{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_existing(data: dict, filepath: str) -> bool:\n",
    "    \"\"\"Compare the current data with the data in an existing file.\n",
    "\n",
    "    If the data is different, replace the file content with the new data.\n",
    "    If the data is the same, update the `last_updated` field in the file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The current data to compare.\n",
    "        filepath (str): The path to the file to compare with.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was updated or replaced, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            return True\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(filepath):\n",
    "            # Load existing data from the file\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                existing_data = json.load(file)\n",
    "\n",
    "            # Compare the new data with the existing data (excluding `accessed`)\n",
    "            existing_data_copy = existing_data.copy()\n",
    "            existing_data_copy.pop(\"accessed\", None)  # Remove `accessed` for comparison\n",
    "            if data == existing_data_copy:\n",
    "                # If data is the same, just update the `accessed` field\n",
    "                existing_data[\"accessed\"] = date.today()\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(existing_data, file, indent=4, ensure_ascii=False)\n",
    "                return True\n",
    "\n",
    "        # If data is different or file doesn't exist, replace it\n",
    "        data[\"accessed\"] = date.today()  # Add timestamp to new data\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing or updating file at {filepath}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_content(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract relevant content from the <main id=\"content_wrapper\"> or <main id=\"site-content\">\n",
    "    section of the page.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the page content.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and relevant content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to find the <main> tag first\n",
    "        main_content = soup.find(\"main\", {\"id\": \"content_wrapper\"})\n",
    "\n",
    "        # If not found, try the <main id=\"site-content\">\n",
    "        if not main_content:\n",
    "            log.warning(\"Main content not found.\")\n",
    "            main_content = soup.find(\"main\", {\"id\": \"site-content\"})\n",
    "\n",
    "        if not main_content:\n",
    "            log.warning(\"Site content not found.\")\n",
    "            main_content = soup.find(\"div\", {\"id\": \"main-body\"})\n",
    "\n",
    "        if not main_content: \n",
    "            log.warning(\"Main-body not found.\")\n",
    "            main_content = soup.find(\"section\", {\"id\": \"sp-main-body\"})\n",
    "\n",
    "        if not main_content:\n",
    "            log.warning(\"Sp-main-body not found.\")\n",
    "            main_content = soup.find(\"div\", {\"id\": \"main\"})\n",
    "\n",
    "        if not main_content:\n",
    "            log.warning(\"Main not found.\")\n",
    "            main_content = soup.find(\"div\", {\"class\": \"sc-gsTCUz bhdLno\"}) \n",
    "\n",
    "        if not main_content:\n",
    "            log.warning(\"sc-gsTCUz bhdLno not found.\")\n",
    "            return \"\"\n",
    "\n",
    "        # Remove unwanted tags like <nav>, <aside>, <script>, etc.\n",
    "        for tag in main_content.find_all([\"nav\", \"aside\", \"script\", \"style\"]):\n",
    "            tag.decompose()  # Remove the tag and its content\n",
    "\n",
    "        # Extract text from the cleaned main content\n",
    "        relevant_text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        return relevant_text\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error extracting relevant content: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(content: dict, filename: str) -> None:\n",
    "    \"\"\"Saves content to a file in JSON format.\n",
    "\n",
    "    Args:\n",
    "        content (dict): The data to save.\n",
    "        filename (str): The filename, including the path, to save the data.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory):\n",
    "        log.warning(f\"Directory '{directory}' does not exist. Creating it.\")\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    log.info(f\"Saving into {filename}.\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(content, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headings = [h.get_text(strip=True) for h in soup.find_all([\"h1\", \"h2\", \"h3\"])]\n",
    "# paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(url: str):\n",
    "    log.info(f\"Starting to extract content from {url}\")\n",
    "    try:    \n",
    "        # Validate the URL\n",
    "        if not url.startswith(\"http\"):\n",
    "            raise ValueError(f\"Invalid URL format: {url}\")\n",
    "\n",
    "        # Fetch the page\n",
    "        page = requests.get(url)\n",
    "        page.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        \n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        text_content = soup.get_text(separator=\"\\n\", strip=True).split(\"\\n\")\n",
    "\n",
    "        # Extract a set of relevant links\n",
    "        links_list = [urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n",
    "        relevant_links_list = [link for link in links_list if \"hdm\" in link]\n",
    "        relevant_links_set = set(relevant_links_list)\n",
    "\n",
    "        # Sanitize filename\n",
    "        sanitized_url = extract_domain_part(url)\n",
    "        if not sanitized_url:\n",
    "            raise ValueError(f\"Unable to extract domain part from URL: {url}\")\n",
    "        filename = f\"{sanitized_url}.txt\"\n",
    "        filename_links = f\"{sanitized_url}_links.txt\"\n",
    "\n",
    "        # Save to files\n",
    "        save_to_file(content=text_content, filename=filename)\n",
    "        save_to_file(content=relevant_links_set, filename=filename_links)\n",
    "\n",
    "        log.info(f\"Finished extracting content\")\n",
    "    \n",
    "    except (requests.RequestException, ValueError) as e:\n",
    "        # Catch network-related errors or invalid URL issues\n",
    "        log.error(f\"Skipping URL due to error: {e}\")\n",
    "    except Exception as e:\n",
    "        # Catch all other unexpected exceptions\n",
    "        log.error(f\"An unexpected error occurred for URL {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.hdm-stuttgart.de/studieninteressierte/studium/bachelor/steckbrief?sgang_ID=550045&sgang_cluster_ID=18\",\n",
    "    \"https://www.hdm-stuttgart.de/studieninteressierte/studium/bachelor/steckbrief?sgang_ID=550045&sgang_cluster_ID=19\",\n",
    "    \"https://www.hdm-stuttgart.de\",\n",
    "    \"https://www.hdm-stuttgart.de/medianight\"\n",
    "]\n",
    "\n",
    "# for url in urls:\n",
    "#     extract_content(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
