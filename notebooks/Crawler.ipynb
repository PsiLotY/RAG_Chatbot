{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m date\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "import os\n",
    "from collections import deque\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "from datetime import date\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = \"chroma\"  # either \"chroma\" or \"locally\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "log = logging.getLogger(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_storage\",\n",
    ")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"hdm_website\",\n",
    "    metadata={\n",
    "        \"description\": \"vectorstore containing hdm website content\",\n",
    "        \"created\": str(date.today()),\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_directory = \"website_data\"\n",
    "extracted_pdf_directory = \"pdf_data\"\n",
    "pdf_directory = \"pdfs\"\n",
    "start_url = \"https://www.hdm-stuttgart.de\"\n",
    "allowed_domains = {\n",
    "    \"hdm-stuttgart.de\",\n",
    "    \"hdm-weiterbildung.de\",\n",
    "    \"vs-hdm.de\",\n",
    "    \"pmt.hdm-stuttgart.de\",\n",
    "    \"omm.hdm-stuttgart.de\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISALLOWED_PATHS = [\n",
    "    \"/studienfuehrer/vorlesungsverzeichnis/\",\n",
    "    \"/studienfuehrer/Studiengaenge/\",\n",
    "    \"/studienfuehrer/dozentenplaene/\",\n",
    "    \"/studienfuehrer/raumbelegung/\",\n",
    "    \"*/manage\",\n",
    "    \"*/manage_main\",\n",
    "    \"*/manage_workspace\",\n",
    "    \"/pdm/pdm_deutsch/\",\n",
    "    \"/pdm/pdm_englisch/\",\n",
    "    \"/pdm/pdm_spanisch/\",\n",
    "    \"*/html2pdf\",\n",
    "    \"*/htmltopdf\",\n",
    "    \"*printview=1\",\n",
    "    \"/pmm/studiengang/team/mitarbeiter/lindig/\",\n",
    "    \"/hochschule/neubau/webcams/tag*\",\n",
    "    \"/ifak/startseite/redaktionzukunft/beitrag.html?beitrag_ID=1817&stars=2\",\n",
    "    \"/*beitrag.html?beitrag_ID=1817\",\n",
    "    \"*view_fotostrecke*\",\n",
    "    \"*hdmnewsmail_simple*\",\n",
    "    \"/vwif/\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_allowed(url: str) -> bool:\n",
    "    \"\"\"Check if a url is allowed based on the disallowed paths.\n",
    "\n",
    "    Args:\n",
    "        url (str): thue url to check\n",
    "\n",
    "    Returns:\n",
    "        bool: True if allowed, False if disallowed\n",
    "    \"\"\"\n",
    "    for path in DISALLOWED_PATHS:\n",
    "        if \"*\" in path:\n",
    "            # Match wildcard patterns\n",
    "            regex_path = path.replace(\"*\", \".*\")\n",
    "            if re.search(regex_path, url):\n",
    "                return False\n",
    "        elif path in url:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_part(url: str) -> str:\n",
    "    \"\"\"Takes in an url and returns a string that is based on the urls domain, path and query.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page being parsed.\n",
    "\n",
    "    Returns:\n",
    "        str: The url in a form thats usable as a path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        # Extract the base domain (e.g., hdm-stuttgart from www.hdm-stuttgart.de)\n",
    "        domain_match = re.search(r\"(?:www\\.)?(.*?)\\.(de|com|org|net|pdf)\", parsed_url.netloc)\n",
    "        base_domain = domain_match.group(1) if domain_match else parsed_url.netloc\n",
    "        # Extract the url path after .de\n",
    "        path = parsed_url.path.strip(\"/\").replace(\"/\", \"_\")\n",
    "        # Extract url query parameters after ?\n",
    "        query = parsed_url.query.replace(\"&\", \"_\").replace(\"=\", \"_\") if parsed_url.query else \"\"\n",
    "\n",
    "        # Combine components\n",
    "        filename = f\"{base_domain}\"\n",
    "        if path:\n",
    "            filename += f\"_{path}\"\n",
    "        if query:\n",
    "            filename += f\"_{query}\"\n",
    "\n",
    "        # Ensure filename is safe\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', \"_\", filename)\n",
    "        return filename or \"default\"  # Fallback if filename is empty\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error generating filename from URL {url}: {e}\")\n",
    "        return \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_content(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract relevant content from the main sections of the page.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the page content.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and relevant content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List of potential tags to search for in priority order\n",
    "        potential_main_tags = [\n",
    "            {\"name\": \"main\", \"attrs\": {\"id\": \"content_wrapper\"}},\n",
    "            {\"name\": \"main\", \"attrs\": {\"id\": \"site-content\"}},\n",
    "            {\"name\": \"div\", \"attrs\": {\"id\": \"main-body\"}},\n",
    "            {\"name\": \"section\", \"attrs\": {\"id\": \"sp-main-body\"}},\n",
    "            {\"name\": \"div\", \"attrs\": {\"id\": \"main\"}},\n",
    "            {\"name\": \"div\", \"attrs\": {\"class\": \"sc-gsTCUz bhdLno\"}},\n",
    "        ]\n",
    "\n",
    "        # Iterate through potential tags until one is found\n",
    "        main_content = None\n",
    "        for tag in potential_main_tags:\n",
    "            main_content = soup.find(tag[\"name\"], tag[\"attrs\"])\n",
    "            if main_content:\n",
    "                break  # Stop searching once a valid tag is found\n",
    "\n",
    "        if not main_content:\n",
    "            log.warning(\"No relevant content section found.\")\n",
    "            return \"\"\n",
    "\n",
    "        # Remove unwanted tags\n",
    "        for tag in main_content.find_all([\"nav\", \"aside\", \"script\", \"style\"]):\n",
    "            tag.decompose()  # Remove the tag and its content\n",
    "\n",
    "        # Extract text from the cleaned main content\n",
    "        relevant_text = main_content.get_text(separator=\" \\n \", strip=True)\n",
    "        # relevant_text = main_content.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        return relevant_text\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error extracting relevant content: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chromadb(url, title, content, doc_type):\n",
    "    \"\"\"Saves data directly to ChromaDB with metadata.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document.\n",
    "        title (str): The title of the document.\n",
    "        content (str): The content of the document.\n",
    "        doc_type (str): The type of document (e.g., 'webpage', 'pdf').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create metadata for the document\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": title,\n",
    "            \"accessed\": str(date.today()),\n",
    "            \"type\": doc_type,\n",
    "            \"url\": url,\n",
    "        }\n",
    "\n",
    "        # Create a unique ID for the document\n",
    "        # hash gives a positive or negative number\n",
    "        doc_id = f\"{doc_type}-{hashlib.sha1(url.encode()).hexdigest()}\"\n",
    "\n",
    "        # Add to ChromaDB\n",
    "        collection.add(\n",
    "            documents=[content],\n",
    "            metadatas=[metadata],\n",
    "            ids=[doc_id],\n",
    "        )\n",
    "\n",
    "        log.info(f\"Document added to ChromaDB: {doc_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to save to ChromaDB for URL {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_content_to_file(url: str, title: str, content: str, file_path: str) -> None:\n",
    "    \"\"\"Saves extracted content to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document.\n",
    "        title (str): The title of the document.\n",
    "        content (str): The content of the document.\n",
    "        file_path (str): The path to save the JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"accessed\": str(date.today()),\n",
    "            \"content\": content,\n",
    "        }\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        log.info(f\"Saved content to {file_path}\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to save content to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_page_content(soup: BeautifulSoup, url: str) -> None:\n",
    "    \"\"\"Saves specific parts of a webpage into a file.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the pag content.\n",
    "        url (str): The URL of the page being parsed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log.info(f\"Saving content from {url}\")\n",
    "        relevant_content = extract_relevant_content(soup)\n",
    "        relevant_content = relevant_content.lower()\n",
    "        if relevant_content == \"\":\n",
    "            log.warning(\"Not saving into file\")\n",
    "            return\n",
    "\n",
    "        title = soup.title.get_text(strip=True) if soup.title else \"No Title\"\n",
    "\n",
    "        sanitized_url = extract_domain_part(url)\n",
    "        filename = os.path.join(webpage_directory, sanitized_url + \".json\")\n",
    "\n",
    "        if save_to == \"chroma\":\n",
    "            save_to_chromadb(url, title, relevant_content, \"webpage\")\n",
    "        elif save_to == \"locally\":\n",
    "            save_content_to_file(url=url, title=title, content=relevant_content, file_path=filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to save content from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_content(response, url: str) -> None:\n",
    "    \"\"\"Process and save PDF content directly from the response.\n",
    "\n",
    "    Args:\n",
    "        response: The HTTP response containing the PDF content.\n",
    "        url (str): The URL of the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the PDF locally\n",
    "        sanitized_url = extract_domain_part(url)\n",
    "        if not sanitized_url.endswith(\".pdf\"):\n",
    "            sanitized_url += \".pdf\"\n",
    "        pdf_filename = os.path.join(pdf_directory, sanitized_url)\n",
    "        json_filename = os.path.join(\n",
    "            extracted_pdf_directory, sanitized_url.replace(\".pdf\", \".json\")\n",
    "        )\n",
    "\n",
    "        with open(pdf_filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        # Extract text from the PDF\n",
    "        reader = PdfReader(pdf_filename)\n",
    "        pdf_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "        pdf_text = pdf_text.replace(\"\\n\", \" \\n \")\n",
    "        pdf_text = pdf_text.lower()\n",
    "\n",
    "        # Extract title from metadata, fallback to filename\n",
    "        metadata_title = reader.metadata.get(\"/Title\", None) if reader.metadata else None\n",
    "        if not isinstance(metadata_title, str):\n",
    "            metadata_title = \"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        filename_title = os.path.basename(parsed_url.path).replace(\".pdf\", \"\")\n",
    "        title = metadata_title or filename_title or \"Untitled PDF\"\n",
    "\n",
    "        if save_to == \"chroma\":\n",
    "            save_to_chromadb(url, title, relevant_content, \"pdf\")\n",
    "        elif save_to == \"locally\":\n",
    "            save_content_to_file(\n",
    "                url=url, title=title, content=relevant_content, file_path=json_filename\n",
    "            )\n",
    "\n",
    "        log.info(f\"Saved PDF and extracted content: {pdf_filename}, {json_filename}\")\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to process PDF {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(soup: BeautifulSoup, url: str, visited: set[str], to_visit: set[str]) -> set[str]:\n",
    "    \"\"\"Extracts all valid links from a webpage.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the pag content.\n",
    "        url (str): The URL of the page being parsed.\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: A set of valid links extracted from the page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract all <a> tags with href attributes\n",
    "        filtered_links = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].strip()\n",
    "            full_url = urljoin(url, href)\n",
    "\n",
    "            if (\n",
    "                \"#\" in full_url\n",
    "                or \"hdm\" not in full_url\n",
    "                or full_url.startswith(\"mailto:\")\n",
    "                or full_url.lower().endswith((\".jpg\", \".png\", \".gif\", \".zip\"))\n",
    "                or not full_url.startswith((\"http://\", \"https://\"))\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            parsed_url = urlparse(full_url)\n",
    "            domain = parsed_url.netloc.lower()\n",
    "            if not any(domain.endswith(allowed) for allowed in allowed_domains):\n",
    "                continue\n",
    "\n",
    "            if full_url not in visited and full_url not in to_visit:\n",
    "                filtered_links.add(full_url)\n",
    "\n",
    "        return filtered_links\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        log.error(f\"Failed to fetch links from {url}: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_visit(to_visit, filename: str = \"to_visit.json\") -> None:\n",
    "    \"\"\"Saves the `to_visit` deque or set into a JSON file.\n",
    "\n",
    "    Args:\n",
    "        to_visit (deque or set): The collection of URLs to save.\n",
    "        filename (str): The file where the URLs will be stored.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert deque to a list if necessary\n",
    "        if isinstance(to_visit, deque):\n",
    "            to_visit = list(to_visit)\n",
    "\n",
    "        # Save to JSON file\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(to_visit, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved {len(to_visit)} URLs to {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to_visit to {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(start_url: str, page_depth: int = None) -> None:\n",
    "    \"\"\"Crawl a website starting from a given URL, timing each iteration.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL to begin crawling from.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    visited = set()\n",
    "    to_visit = deque([start_url])\n",
    "    to_visit_set = set([start_url])\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    if not os.path.isdir(webpage_directory):\n",
    "        log.info(f\"Directory {webpage_directory} doesn't exist, is being created.\")\n",
    "        os.mkdir(webpage_directory)\n",
    "\n",
    "    if not os.path.isdir(extracted_pdf_directory):\n",
    "        log.info(f\"Directory {extracted_pdf_directory} doesn't exist, is being created.\")\n",
    "        os.mkdir(extracted_pdf_directory)\n",
    "\n",
    "    if not os.path.isdir(pdf_directory):\n",
    "        log.info(f\"Directory {pdf_directory} doesn't exist, is being created.\")\n",
    "        os.mkdir(pdf_directory)\n",
    "\n",
    "    while to_visit:\n",
    "        iteration_start_time = time.time()  # Start timing the iteration\n",
    "        current_url = to_visit.popleft()\n",
    "        to_visit_set.remove(current_url)\n",
    "\n",
    "        if current_url in visited:\n",
    "            continue\n",
    "\n",
    "        if not is_allowed(current_url):\n",
    "            log.warning(f\"Skipping disallowed URL: {current_url}\")\n",
    "            continue\n",
    "\n",
    "        log.info(f\"Links visited: {len(visited)}, Links left to visit: {len(to_visit)}\")\n",
    "        log.info(f\"Visiting: {current_url}\")\n",
    "        try:\n",
    "            # Request the website\n",
    "            start_request = time.time()\n",
    "            response = session.get(current_url, timeout=10)\n",
    "            end_request = time.time()\n",
    "            log.info(f\"Fetching {current_url} took {end_request - start_request:.4f} seconds\")\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Check if the content is a PDF\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "            log.info(f\"Content type: {content_type}\")\n",
    "            if \"text/html\" in content_type:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                # Get the content and save it to file\n",
    "                save_page_content(soup, current_url)\n",
    "\n",
    "                # Extract the links\n",
    "                new_links = extract_links(\n",
    "                    soup=soup, url=current_url, visited=visited, to_visit=to_visit_set\n",
    "                )\n",
    "                for link in new_links:\n",
    "                    if link not in visited and link not in to_visit_set:\n",
    "                        to_visit.append(link)\n",
    "                        to_visit_set.add(link)\n",
    "\n",
    "            elif \"application/pdf\" in content_type:\n",
    "                log.info(f\"Detected PDF: {current_url}\")\n",
    "                process_pdf_content(response, current_url)  # Process PDF directly from the response\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                log.error(\n",
    "                    f\"Current URL {current_url} is of unsupported content type {content_type}.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            visited.add(current_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"Failed to process URL {current_url}: {e}\")\n",
    "\n",
    "        if page_depth is not None:\n",
    "            i += 1\n",
    "            if i >= page_depth:\n",
    "                log.info(\"Reached maximum page depth, stopping.\")\n",
    "                break\n",
    "\n",
    "        iteration_end_time = time.time()  # End timing the iteration\n",
    "        elapsed_time = iteration_end_time - iteration_start_time\n",
    "        log.info(f\"Iteration {i} completed in {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        delay = random.uniform(0.5, 2)\n",
    "        log.info(f\"Sleeping for {delay:.2f} seconds\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    save_to_visit(to_visit)\n",
    "    log.info(\"Crawling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_website(start_url)\n",
    "# crawl_website(start_url, page_depth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_url = \"mailto:friese@hdm-stuttgart.de\"\n",
    "# test_url = \"https://curdt.home.hdm-stuttgart.de/PDF/Vogler.pdf\"\n",
    "# test_url = \"https://www.hdm-stuttgart.de/hochschule/organisation/rektorat\"\n",
    "# crawl_website(test_url, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_website_data():\n",
    "    \"\"\"Clean up files in /website_data by deleting those with empty content fields in their JSON.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(webpage_directory):\n",
    "            log.warning(f\"Directory {webpage_directory} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Iterate through all files in the directory\n",
    "        for filename in os.listdir(webpage_directory):\n",
    "            file_path = os.path.join(webpage_directory, filename)\n",
    "\n",
    "            # Only process JSON files\n",
    "            if filename.endswith(\".json\"):\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        data = json.load(file)\n",
    "\n",
    "                    # Check if the \"content\" field is empty\n",
    "                    if not data.get(\"content\", \"\").strip():\n",
    "                        log.info(f\"Deleting {file_path} due to empty content.\")\n",
    "                        os.remove(file_path)\n",
    "                except (json.JSONDecodeError, FileNotFoundError, PermissionError) as e:\n",
    "                    log.error(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Failed to clean up website data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
